{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6 - Benchmark classification on ```cifar-10```\n",
    "\n",
    "This notebook builds on what we were doing last week with the handwritten digits from the MNIST dataset.\n",
    "\n",
    "This week, we're working with another famous dataset in computer vision and image processing research - [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029515af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 12:24:10.650672: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 12:24:10.804715: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/coder/.local/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-10 12:24:10.804737: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-10 12:24:11.429286: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/coder/.local/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-10 12:24:11.429416: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/coder/.local/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-03-10 12:24:11.429423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# path tools\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# data loader\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# machine learning tools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# classificatio models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224dc18b",
   "metadata": {},
   "source": [
    "We're going to load the data using a function from the library ```TensorFlow```, which we'll be looking at in more detail next week. \n",
    "\n",
    "For now, we're just using it to fetch the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b2ec52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e25f0",
   "metadata": {},
   "source": [
    "**Question:** What is the shape of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf0966b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # fourth dimension is the number of types we have of the other three color channels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0876162",
   "metadata": {},
   "source": [
    "Unfortunately, this version of the data set doesn't have explict labels, so we need to create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2402554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"airplane\", \n",
    "          \"automobile\", \n",
    "          \"bird\", \n",
    "          \"cat\", \n",
    "          \"deer\", \n",
    "          \"dog\", \n",
    "          \"frog\", \n",
    "          \"horse\", \n",
    "          \"ship\", \n",
    "          \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb7ae2b",
   "metadata": {},
   "source": [
    "### Convert all the data to greyscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d043f9d",
   "metadata": {},
   "source": [
    "In the following cell, I'm converting all of my images to greyscale and then making a ```numpy``` array at the end.\n",
    "\n",
    "Notice that I'm using something funky here called *[list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658dcd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for list comprehensions \n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "uppers = []\n",
    "\n",
    "for color in colors:\n",
    "    upper = color.upper() # make all upper case \n",
    "    uppers.append(upper) # append to the empty list \n",
    "# can also compress that into one line of code in the for loop \n",
    "\n",
    "# how to make this example into a list comprehension \n",
    "# set up is, do this thing for everything in the list - where as the for loop is the opposite, for this list do this thing \n",
    "# it is all in [] so don't need to append to a new list because it's already in a new list immediately \n",
    "uppers = [color.upper for color in colors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886fc12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_grey = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_train])\n",
    "X_test_grey = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2997c",
   "metadata": {},
   "source": [
    "Can use this \"nicely\" by making a function with multiple steps and then call the function and apply to data in a list comprehension. Otherwise if there are multiple steps in a for loop, converting to a list comprehension can get very confusing  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a78856",
   "metadata": {},
   "source": [
    "Then, we're going to do some simple scaling by dividing by 255. Which rescales all the pixel values, still 32x32 and 50,000 images but just scales down the pixel values from 0-255 to 0-1. Also gives small changes in weights a more pronounced affect  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086c324",
   "metadata": {},
   "source": [
    "Rescaling makes it faster and easier to converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2980d346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_scaled = (X_train_grey)/255.0\n",
    "X_test_scaled = (X_test_grey)/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2505c",
   "metadata": {},
   "source": [
    "### Reshaping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173cd79",
   "metadata": {},
   "source": [
    "Next, we're going to reshape this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c3b51f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_train_scaled.shape # returning the shape of each object, nsamples = 50,000, nx(x axis values) = 32, ny = 32\n",
    "X_train_dataset = X_train_scaled.reshape((nsamples,nx*ny)) # new shape is the number of values 50,000 and the second set is 32*32 which flattens everything into 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb81ff22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_test_scaled.shape\n",
    "X_test_dataset = X_test_scaled.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c382e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1024)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dataset.shape # which means are input data is now ready to go into classifier models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd80b7",
   "metadata": {},
   "source": [
    "## Simple logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171afb1",
   "metadata": {},
   "source": [
    "We define our Logistic Regression classifier as we have done previously. You'll notice that I've set a lot of different parameters here - you can learn more in the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "penalty - use all the weights, learn as much as possible, no penalty. If have a penalty, set some weights to 0 and only use important ones..?\n",
    "\n",
    "tol - tolerance, by how much weights should be changing when it updates every time, if it doesn't do this over about 10 iterations it will stop because model is no longer improving  \n",
    "\n",
    "verbose - if false no output/update on how model is performing \n",
    "\n",
    "solver - see documentation for suggestions, for larger datasets, sag and saga and for multinomial datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b763f08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.24794480\n",
      "Epoch 3, change: 0.16788796\n",
      "convergence after 4 epochs took 10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.2s finished\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty=\"none\", \n",
    "                        tol=0.1, \n",
    "                        verbose=True, \n",
    "                        solver=\"saga\",\n",
    "                        multi_class=\"multinomial\").fit(X_train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then print our classification report, using the label names that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.34      0.37      0.35      1000\n",
      "  automobile       0.37      0.39      0.38      1000\n",
      "        bird       0.27      0.22      0.24      1000\n",
      "         cat       0.24      0.14      0.18      1000\n",
      "        deer       0.26      0.19      0.22      1000\n",
      "         dog       0.29      0.33      0.31      1000\n",
      "        frog       0.27      0.37      0.31      1000\n",
      "       horse       0.33      0.30      0.31      1000\n",
      "        ship       0.35      0.41      0.37      1000\n",
      "       truck       0.39      0.45      0.42      1000\n",
      "\n",
      "    accuracy                           0.32     10000\n",
      "   macro avg       0.31      0.32      0.31     10000\n",
      "weighted avg       0.31      0.32      0.31     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, \n",
    "                               y_pred, \n",
    "                               target_names=labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c6084",
   "metadata": {},
   "source": [
    "Start by setting a benchmark, start simple like with Logistic Regression. See how it does and can it beat this target with ML or neutral network stuff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've set a couple of different parameters here - you can see more in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "**NB!** This will take a long time to run! On the 32 CPU machine on UCloud, this takes around 30 seconds per iteration.\n",
    "\n",
    "learning_rate=\"adaptive\" <-- want it to learn quickly and once weights are set then go more slowly and carefully. Can adapt it's learning rate by learning at different speeds \n",
    "\n",
    "early_stopping - if we change the tol level from before - connected to this. Not totally sure about this part \n",
    "\n",
    "output, validation score - training model on training data, then it check accuracy on validation dataset which gives a score and does this for 20 iteratations and then evalulates how it does on the totally unseen test data \n",
    "in training - learns loss values from training data, training model my minimizing loss \n",
    "Validataion dataset is a very small subsection of the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30872956\n",
      "Validation score: 0.133000\n",
      "Iteration 2, loss = 2.15971661\n",
      "Validation score: 0.239200\n",
      "Iteration 3, loss = 2.02581278\n",
      "Validation score: 0.265200\n",
      "Iteration 4, loss = 1.97076182\n",
      "Validation score: 0.281800\n",
      "Iteration 5, loss = 1.93555578\n",
      "Validation score: 0.302600\n",
      "Iteration 6, loss = 1.90926190\n",
      "Validation score: 0.315600\n",
      "Iteration 7, loss = 1.89160286\n",
      "Validation score: 0.318800\n",
      "Iteration 8, loss = 1.87500641\n",
      "Validation score: 0.322200\n",
      "Iteration 9, loss = 1.86730610\n",
      "Validation score: 0.316800\n",
      "Iteration 10, loss = 1.85845283\n",
      "Validation score: 0.321200\n",
      "Iteration 11, loss = 1.84549829\n",
      "Validation score: 0.331400\n",
      "Iteration 12, loss = 1.83590762\n",
      "Validation score: 0.328600\n",
      "Iteration 13, loss = 1.82908945\n",
      "Validation score: 0.331400\n",
      "Iteration 14, loss = 1.82320985\n",
      "Validation score: 0.330600\n",
      "Iteration 15, loss = 1.81056794\n",
      "Validation score: 0.343400\n",
      "Iteration 16, loss = 1.80707784\n",
      "Validation score: 0.338400\n",
      "Iteration 17, loss = 1.79877427\n",
      "Validation score: 0.339800\n",
      "Iteration 18, loss = 1.79244407\n",
      "Validation score: 0.351000\n",
      "Iteration 19, loss = 1.78417279\n",
      "Validation score: 0.348800\n",
      "Iteration 20, loss = 1.78163463\n",
      "Validation score: 0.363800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=42,\n",
    "                    hidden_layer_sizes=(64, 10), # two hidden layers, 64 and then 10 - gets better if change to 100, 10 which increases complexity of the architecture \n",
    "                    learning_rate=\"adaptive\",\n",
    "                    early_stopping=True,\n",
    "                    verbose=True,\n",
    "                    max_iter=20).fit(X_train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can get our classification report as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.38      0.41      0.40      1000\n",
      "  automobile       0.40      0.49      0.44      1000\n",
      "        bird       0.26      0.34      0.30      1000\n",
      "         cat       0.28      0.11      0.16      1000\n",
      "        deer       0.27      0.26      0.27      1000\n",
      "         dog       0.33      0.34      0.34      1000\n",
      "        frog       0.28      0.29      0.28      1000\n",
      "       horse       0.45      0.39      0.42      1000\n",
      "        ship       0.44      0.44      0.44      1000\n",
      "       truck       0.42      0.47      0.44      1000\n",
      "\n",
      "    accuracy                           0.35     10000\n",
      "   macro avg       0.35      0.35      0.35     10000\n",
      "weighted avg       0.35      0.35      0.35     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, \n",
    "                               y_pred, \n",
    "                               target_names=labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Take the code outlined in this notebook and turn it into two separate Python scripts, one which performs Logistic Regression classification and one which uses the MLPClassifier on the ```Cifar10``` dataset.\n",
    "\n",
    "Try to use the things we've spoken about in clas\n",
    "- Requirements.txt\n",
    "- Virtual environment\n",
    "- Setup scripts\n",
    "- Argparse\n",
    "\n",
    "This task is [Assignment 2 for Visual Analytics](https://classroom.github.com/a/KLVvny7d)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
